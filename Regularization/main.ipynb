{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __REGULARIZATION__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization is a way to reduce the Model Overfitting and Variance\n",
    "\n",
    "- It is a form of regression that shrinks the coefficient estimates towards zero. In other words, this technique forces us not to learn a more            complex or flexible model, to avoid the problem of overfitting.\n",
    "\n",
    "- Regularization works by adding a penalty or complexity term or shrinkage term with Residual Sum of Squares (RSS) to the complex model.\n",
    "\n",
    "- It solves :\n",
    "    - Minimizing model complexity\n",
    "    - Penalizing the Loss Function\n",
    "    - Reducing Model overfitting\n",
    "\n",
    "- We have 3 main types of Regularization :\n",
    "    1. L1 regularization (Lasso Regression)\n",
    "    2. L2 Regularization (Ridge Regression)\n",
    "    3. Elastic Net (Combining L1 and L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIDGE REGRESSION\n",
    "\n",
    "- Also called as L2 Regularization\n",
    "\n",
    "- It is used to reduce __Over fitting__\n",
    "\n",
    "- In order to avoid over fitting we should change __best fit line__ and to do this we need __Ridge Regression__\n",
    "\n",
    "- Ridge regression is a regularization technique that works by helping reduce the potential for overfitting to the training data\n",
    "\n",
    "- It does this by adding in a penalty term to the error that is based on the squared value of the coefficients\n",
    "\n",
    "- L2 regularization adds a penalty equal to the square of the magnitude of coefficients\n",
    "\n",
    "- Goal of Ridge regression is to add penalty called ***Shrinkage Penalty***!SECTION\n",
    "\n",
    "- Shrinkage penalty is based off the squared coefficient\n",
    "\n",
    "- Shrinking the coefficients estimates can signficantly reduce the Variance\n",
    "\n",
    "- All coefficients are shrunk by some factor. It doesn't necessarily eliminates coefficients\n",
    "\n",
    "![](./Images/ridge.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIDGE REGRESSION\n",
    "\n",
    "- Also called as L1 Regularization\n",
    "\n",
    "- Used for Feature Selection\n",
    "\n",
    "- L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients\n",
    "\n",
    "    - Limits the size of the coefficients \n",
    "\n",
    "- This can yield models where some coefficients can become zero\n",
    "\n",
    "- Lasso can force some of the coefficient estimates to be exacylt equal to zero whnen the tuning parameters lambda is sufficiently large\n",
    "\n",
    "- Coefficients are likely to be zero. So, features which are not highly correlated or least important will be deleted automatically\n",
    "\n",
    "![](./Images/lasso.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELASTIC NET\n",
    "\n",
    "- It is combination of both L1 and L2\n",
    "\n",
    "![](./Images/elastic.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These regularization methods do have a cost\n",
    "\n",
    "- Introduce an additional hyperparameter that needs to be tunes\n",
    "\n",
    "- A multipier to the penalty to decide the 'strength' of the penalty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
